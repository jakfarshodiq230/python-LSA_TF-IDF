{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis\n",
    "\n",
    "In this notebook we're going to look at how we can 'mine' concepts from a corpus (collection) of text documents.\n",
    "\n",
    "In the first week of class everyone wrote their own definition of data science.   This week I'm going to show you how to extract 'concepts' from that corpus mathematically.  The techinque we're going to use is called latent semantic analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jakfar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I exported the forum posts for module 0 into an XML file.  Each post is wrapped in <text></text> tags.   I'll use BeautifulSoup to process the XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = open('raw_forum_posts.dat', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-0657b055bb8d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lxml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpostTxt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#all posts <text>\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpostDocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpostTxt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpostDocs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpostDocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpostDocs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jakfar\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[1;34m\"Couldn't find a tree builder with the features you \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                     \u001b[1;34m\"requested: %s. Do you need to install a parser library?\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                     % \",\".join(features))\n\u001b[0m\u001b[0;32m    197\u001b[0m             \u001b[0mbuilder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuilder_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m             if not (original_features == builder.NAME or\n",
      "\u001b[1;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(posts, 'lxml')\n",
    "postTxt = soup.findAll('text')  #all posts <text> \n",
    "postDocs = [x.text for x in postTxt]\n",
    "postDocs.pop(0)\n",
    "postDocs = [x.lower() for x in postDocs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords are words that I don't want to convert to featurs,becuase they aren't especially useful.  Words like 'a', 'and', and 'the' are good stopwords in english.   I can use a built in list of stopwords from nltk to get me started.  Then, I'll add some custom stopwords that are 'html junk' that I need to clean out of my data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopset = set(stopwords.words('english'))\n",
    "stopset.update(['lt','p','/p','br','amp','quot','field','font','normal','span','0px','rgb','style','51', \n",
    "                'spacing','text','helvetica','size','family', 'space', 'arial', 'height', 'indent', 'letter'\n",
    "                'line','none','sans','serif','transform','line','variant','weight','times', 'new','strong', 'video', 'title'\n",
    "                'white','word','letter', 'roman','0pt','16','color','12','14','21', 'neue', 'apple', 'class',  ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizing\n",
    "\n",
    "I'm going to use scikit-learn's TF-IDF vectorizer to take my corpus and convert each document into a sparse matrix of TFIDF Features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>data science is about analyzing relevant data to obtain patterns of information in order to help achieve a goal. the main focus of the data analysis is the goal rather then the methodology on how it will achieved. this allows for creative thinking and allowing for the optimal solution or model to be found wihtout the constraint of a specific methodology.</p>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Before!\n",
    "postDocs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stopset,\n",
    "                                 use_idf=True, ngram_range=(1, 3))\n",
    "X = vectorizer.fit_transform(postDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x3367 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 89 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 553)\t0.10905143902\n",
      "  (0, 3301)\t0.10905143902\n",
      "  (0, 1290)\t0.10905143902\n",
      "  (0, 1940)\t0.10905143902\n",
      "  (0, 2747)\t0.10905143902\n",
      "  (0, 2069)\t0.10905143902\n",
      "  (0, 107)\t0.10905143902\n",
      "  (0, 2969)\t0.10905143902\n",
      "  (0, 620)\t0.10905143902\n",
      "  (0, 110)\t0.10905143902\n",
      "  (0, 54)\t0.10905143902\n",
      "  (0, 1906)\t0.10905143902\n",
      "  (0, 2369)\t0.10905143902\n",
      "  (0, 1384)\t0.10905143902\n",
      "  (0, 147)\t0.10905143902\n",
      "  (0, 650)\t0.10905143902\n",
      "  (0, 1254)\t0.10905143902\n",
      "  (0, 1805)\t0.10905143902\n",
      "  (0, 1380)\t0.10905143902\n",
      "  (0, 49)\t0.10905143902\n",
      "  (0, 1463)\t0.10905143902\n",
      "  (0, 2079)\t0.10905143902\n",
      "  (0, 1589)\t0.10905143902\n",
      "  (0, 2146)\t0.10905143902\n",
      "  (0, 2030)\t0.10905143902\n",
      "  :\t:\n",
      "  (0, 1284)\t0.0882799340268\n",
      "  (0, 1932)\t0.0761293825223\n",
      "  (0, 2743)\t0.0969008875155\n",
      "  (0, 2067)\t0.10905143902\n",
      "  (0, 105)\t0.10905143902\n",
      "  (0, 2967)\t0.0969008875155\n",
      "  (0, 618)\t0.0969008875155\n",
      "  (0, 108)\t0.10905143902\n",
      "  (0, 52)\t0.10905143902\n",
      "  (0, 1904)\t0.21810287804\n",
      "  (0, 2367)\t0.10905143902\n",
      "  (0, 143)\t0.071509957232\n",
      "  (0, 1250)\t0.0882799340268\n",
      "  (0, 1803)\t0.10905143902\n",
      "  (0, 1378)\t0.176559868054\n",
      "  (0, 47)\t0.0969008875155\n",
      "  (0, 1461)\t0.0761293825223\n",
      "  (0, 2073)\t0.0761293825223\n",
      "  (0, 1577)\t0.0529592419308\n",
      "  (0, 2142)\t0.0969008875155\n",
      "  (0, 2028)\t0.10905143902\n",
      "  (0, 2402)\t0.0882799340268\n",
      "  (0, 160)\t0.0675084290336\n",
      "  (0, 2473)\t0.0310567745199\n",
      "  (0, 642)\t0.0899008417366\n"
     ]
    }
   ],
   "source": [
    "#After\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA\n",
    "\n",
    "Input:  X, a matrix where m is the number of documents I have, and n is the number of terms.\n",
    "\n",
    "Process:   I'm going to decompose X into three matricies called U, S, and T.  When we do the decomposition, we have to pick a value k, that's how many concepts we are going to keep.  \n",
    "\n",
    "$$X \\approx USV^{T}$$\n",
    "\n",
    "U will be a m x k matrix.  The rows will be documents and the columns will be 'concepts'\n",
    "\n",
    "S will be a k x k diagnal matrix.   The elements will be the amount of variation captured from each concept.\n",
    "\n",
    "V will be a n x k (mind the transpose) matrix.   The rows will be terms and the columns will be conepts.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 3367)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=27, n_iter=100,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa = TruncatedSVD(n_components=27, n_iter=100)\n",
    "lsa.fit(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00524647,  0.00524647,  0.00524647, ...,  0.00435294,\n",
       "        0.00435294,  0.00435294])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is the first row for V\n",
    "lsa.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.1 |Anaconda 4.0.0 (64-bit)| (default, Dec  7 2015, 11:16:01) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept 0:\n",
      "data\n",
      "procedures\n",
      "large amounts\n",
      "large amounts data\n",
      "science\n",
      "amounts\n",
      "amounts data\n",
      "different\n",
      "could\n",
      "large\n",
      " \n",
      "Concept 1:\n",
      "procedures\n",
      "large amounts\n",
      "large amounts data\n",
      "could\n",
      "amounts\n",
      "amounts data\n",
      "large\n",
      "used\n",
      "according data\n",
      "according data science\n",
      " \n",
      "Concept 2:\n",
      "white\n",
      "converted\n",
      "make\n",
      "white converted\n",
      "white white\n",
      "20 white ata\n",
      "according\n",
      "47 20 white\n",
      "methods\n",
      "perspective\n",
      " \n",
      "Concept 3:\n",
      "white\n",
      "questions\n",
      "answer\n",
      "using\n",
      "part\n",
      "canada\n",
      "contacts\n",
      "way\n",
      "since\n",
      "efficient\n",
      " \n",
      "Concept 4:\n",
      "business\n",
      "statistics\n",
      "fields\n",
      "knowledge\n",
      "complex\n",
      "gained\n",
      "opinion\n",
      "statistics data\n",
      "studies\n",
      "use statistics\n",
      " \n",
      "Concept 5:\n",
      "dig\n",
      "white\n",
      "business\n",
      "methods\n",
      "perspective\n",
      "archaeology\n",
      "history\n",
      "many\n",
      "different\n",
      "others\n",
      " \n",
      "Concept 6:\n",
      "different\n",
      "provide\n",
      "child\n",
      "collect\n",
      "lego\n",
      "ways\n",
      "scientist\n",
      "collect data\n",
      "data sets\n",
      "feedback\n",
      " \n",
      "Concept 7:\n",
      "dig\n",
      "archaeology\n",
      "history\n",
      "analysis\n",
      "find\n",
      "computer\n",
      "organization\n",
      "whether\n",
      "read\n",
      "decisions\n",
      " \n",
      "Concept 8:\n",
      "large\n",
      "business\n",
      "competitive edge\n",
      "edge\n",
      "especially\n",
      "questions\n",
      "competitive\n",
      "data scientist\n",
      "technologies\n",
      "scientist\n",
      " \n",
      "Concept 9:\n",
      "digital\n",
      "trends\n",
      "knowledge\n",
      "need\n",
      "business\n",
      "world\n",
      "software\n",
      "age\n",
      "age information\n",
      "age information recorded\n",
      " \n",
      "Concept 10:\n",
      "scientist\n",
      "statistics data\n",
      "studies\n",
      "use statistics\n",
      "use statistics data\n",
      "problems\n",
      "use\n",
      "almost\n",
      "way\n",
      "common\n",
      " \n",
      "Concept 11:\n",
      "ability explain\n",
      "ability\n",
      "videos\n",
      "could\n",
      "questions\n",
      "since\n",
      "answers\n",
      "valuable\n",
      "often\n",
      "white\n",
      " \n",
      "Concept 12:\n",
      "make better\n",
      "techniques\n",
      "ultimately\n",
      "greater\n",
      "humanity\n",
      "hi\n",
      "available\n",
      "data available\n",
      "skills\n",
      "order\n",
      " \n",
      "Concept 13:\n",
      "large amounts\n",
      "large amounts data\n",
      "white\n",
      "analyzing data\n",
      "amounts data help\n",
      "analyzing data especially\n",
      "bi\n",
      "bi experts\n",
      "bi experts excels\n",
      "business intelligent\n",
      " \n",
      "Concept 14:\n",
      "use\n",
      "ability predict pretty\n",
      "47\n",
      "could\n",
      "data science\n",
      "use data\n",
      "technologies\n",
      "trying\n",
      "data\n",
      "digital\n",
      " \n",
      "Concept 15:\n",
      "ability make intelligent\n",
      "used\n",
      "competitive edge\n",
      "edge\n",
      "especially\n",
      "tools\n",
      "thank\n",
      "business\n",
      "analyzing\n",
      "come\n",
      " \n",
      "Concept 16:\n",
      "used\n",
      "ability\n",
      "decisions\n",
      "practices\n",
      "ability predict pretty\n",
      "problem\n",
      "ability make\n",
      "answer\n",
      "make\n",
      "akhilesh\n",
      " \n",
      "Concept 17:\n",
      "20 white ata\n",
      "ability make\n",
      "ability explain\n",
      "47 20\n",
      "hello\n",
      "since\n",
      "data\n",
      "30 million\n",
      "information\n",
      "thank\n",
      " \n",
      "Concept 18:\n",
      "large amounts\n",
      "large amounts data\n",
      "different\n",
      "information\n",
      "30 million\n",
      "ability explain understand\n",
      "part\n",
      "tools\n",
      "analysis\n",
      "model\n",
      " \n",
      "Concept 19:\n",
      "ability make\n",
      "ability make intelligent\n",
      "ability explain understand\n",
      "47\n",
      "large\n",
      "way\n",
      "since\n",
      "used\n",
      "ability\n",
      "could\n",
      " \n",
      "Concept 20:\n",
      "information\n",
      "ability predict\n",
      "data science\n",
      "questions\n",
      "figure\n",
      "since\n",
      "science\n",
      "amounts\n",
      "amounts data\n",
      "model\n",
      " \n",
      "Concept 21:\n",
      "ability explain\n",
      "data science\n",
      "ability predict pretty\n",
      "ability explain understand\n",
      "30 million pages\n",
      "could\n",
      "answer\n",
      "according\n",
      "science\n",
      "thanks\n",
      " \n",
      "Concept 22:\n",
      "47 20 white\n",
      "ability predict\n",
      "47\n",
      "ability predict pretty\n",
      "different\n",
      "hello\n",
      "days\n",
      "20\n",
      "ability make intelligent\n",
      "technologies\n",
      " \n",
      "Concept 23:\n",
      "ability\n",
      "science\n",
      "47\n",
      "30 million pages\n",
      "analysis\n",
      "ability make intelligent\n",
      "ability explain\n",
      "information\n",
      "20 white\n",
      "thank\n",
      " \n",
      "Concept 24:\n",
      "ability predict\n",
      "30 million\n",
      "ability\n",
      "ability predict pretty\n",
      "data science\n",
      "data\n",
      "30\n",
      "different\n",
      "47\n",
      "ability make\n",
      " \n",
      "Concept 25:\n",
      "47 20\n",
      "ability predict pretty\n",
      "20 white\n",
      "science\n",
      "questions\n",
      "business\n",
      "thank\n",
      "practices\n",
      "model\n",
      "scientist\n",
      " \n",
      "Concept 26:\n",
      "20\n",
      "30 million\n",
      "science\n",
      "data\n",
      "large\n",
      "ability predict pretty\n",
      "analysis\n",
      "information\n",
      "data science\n",
      "47 20 white\n",
      " \n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "for i, comp in enumerate(lsa.components_): \n",
    "    termsInComp = zip (terms,comp)\n",
    "    sortedTerms =  sorted(termsInComp, key=lambda x: x[1], reverse=True) [:10]\n",
    "    print(\"Concept %d:\" % i )\n",
    "    for term in sortedTerms:\n",
    "        print(term[0])\n",
    "    print (\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.24646626e-03,   5.24646626e-03,   5.24646626e-03, ...,\n",
       "          4.35293786e-03,   4.35293786e-03,   4.35293786e-03],\n",
       "       [ -9.94441560e-03,  -9.94443477e-03,  -9.94448909e-03, ...,\n",
       "         -7.47511672e-03,  -7.47511672e-03,  -7.47511781e-03],\n",
       "       [  3.24430859e-02,   3.36540548e-02,   5.96254890e-02, ...,\n",
       "         -5.67205587e-03,  -5.67205587e-03,  -5.56974736e-03],\n",
       "       ..., \n",
       "       [ -1.15545114e-02,  -3.33576033e-01,  -2.17246552e-01, ...,\n",
       "         -3.25054705e-04,  -3.25054705e-04,  -2.14573508e-03],\n",
       "       [ -2.55995170e-02,   1.89322338e-01,  -2.38088432e-01, ...,\n",
       "          8.85681690e-04,   8.85681690e-04,  -1.13710361e-03],\n",
       "       [  4.94312104e-01,  -2.37995590e-02,  -3.26411566e-01, ...,\n",
       "         -8.96565955e-04,  -8.96565955e-04,  -1.28023812e-03]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
