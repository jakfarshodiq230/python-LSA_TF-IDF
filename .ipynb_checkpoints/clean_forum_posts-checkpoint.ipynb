{
 "metadata": {
  "name": "",
  "signature": "sha256:b00bf8407e2a269a2d0e8eedb105f7bf50a35c8db1ed8f772887e9fa66931d54"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from BeautifulSoup import BeautifulSoup\n",
      "import re\n",
      "import nltk\n",
      "from nltk import word_tokenize\n",
      "from __future__ import division\n",
      "from nltk.corpus import stopwords\n",
      "import string\n",
      "from nltk.stem import SnowballStemmer\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 122
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "posts = open('raw_forum_posts.dat', 'rw').read()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "soup = BeautifulSoup(posts)\n",
      "postTxt = soup.findAll('text')  #all posts <text> \n",
      "postDocs = [x.text for x in postTxt]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 106
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "emptyOne = postDocs.pop(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 114
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tokenizePosts(postDocs):\n",
      "    tokenizedDocs = []\n",
      "    #stopwords\n",
      "    stopset = set(stopwords.words('english'))\n",
      "    stopset.update(['lt','p','/p'])\n",
      "    tokenizedDocs = []\n",
      "    \n",
      "    for doc in postDocs:\n",
      "        words = word_tokenize(doc)\n",
      "        filtered_words = [w for w in words if not w in stopset]\n",
      "        filtered_words = [w for w in filtered_words if not w in string.punctuation]\n",
      "        tokenizedDocs.append(filtered_words)\n",
      "    return tokenizedDocs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 115
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokenizedDocs = tokenizePosts(postDocs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 116
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def stem_tokens(tokens):\n",
      "    stemmed = []\n",
      "    stemmer = SnowballStemmer('english')\n",
      "    for item in tokens:\n",
      "        stemmed.append(str(stemmer.stem(item)))\n",
      "    return stemmed\n",
      "\n",
      "def StemTokens(tokenizedDocs):\n",
      "    stemmedDocs = []\n",
      "    for doc in tokenizedDocs:\n",
      "        stemmedDocs.append(stem_tokens(doc))\n",
      "    return stemmedDocs\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 136
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stemmedDocs = StemTokens(tokenizedDocs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 137
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os import path\n",
      "import matplotlib.pyplot as plt\n",
      "from wordcloud import WordCloud"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 139
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}