{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:37.342730Z",
     "start_time": "2019-11-04T14:10:10.616201Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:37.639747Z",
     "start_time": "2019-11-04T14:10:37.346730Z"
    }
   },
   "outputs": [],
   "source": [
    "#update data stopwords\n",
    "stopset = set(stopwords.words('indonesian'))\n",
    "stopset.update(['jika','nya','orang','kurangnya','baiknya','berkali','kali','mata', 'olah', \n",
    "                'sekurang', 'setidak', 'tama', 'tidaknya','al','quran', 'allah','kitab','turun','simpang','sungguh',\n",
    "                'bawa','selisih','berita','gembira','izin','musuh','barang','bumi','ampun','hitung','kuasa','lahir',\n",
    "                'langit','niscaya','sembunyi','hati','saksi','adil','bapa','biarpun','enggan','hawa','kaum','kaya',\n",
    "               'kerja','maslahat','hadap','miskin','nafsu','putar','tegak','kerabat','kecuali','nyala'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.136775Z",
     "start_time": "2019-11-04T14:10:37.649747Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'AlquranCoba.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1056867b34ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#proses memebaca file csv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mposts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'AlquranCoba.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m';'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'AlquranCoba.csv'"
     ]
    }
   ],
   "source": [
    "#proses memebaca file csv\n",
    "posts = pd.read_csv(open('AlquranCoba.csv', newline='', encoding='utf-8'), delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.160777Z",
     "start_time": "2019-11-04T14:10:10.632Z"
    }
   },
   "outputs": [],
   "source": [
    "#menampilkan dari fle csv\n",
    "print (posts.shape)\n",
    "posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.164777Z",
     "start_time": "2019-11-04T14:10:10.637Z"
    }
   },
   "outputs": [],
   "source": [
    "#menamplkan nama index kolom csv\n",
    "print (list(posts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.168777Z",
     "start_time": "2019-11-04T14:10:10.641Z"
    }
   },
   "outputs": [],
   "source": [
    "#menamplkan isi nama index kolom tertentu csv\n",
    "example_review = posts.iloc[0]\n",
    "print(example_review['Terjemahan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.170777Z",
     "start_time": "2019-11-04T14:10:10.645Z"
    }
   },
   "outputs": [],
   "source": [
    "#proses tokenizng\n",
    "def identify_tokens(row):\n",
    "    review = row['Terjemahan']\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    # taken only words (not punctuation)\n",
    "    token_words = [w for w in tokens if w.isalpha()]\n",
    "    return token_words\n",
    "\n",
    "posts['words'] = posts.apply(identify_tokens, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.171777Z",
     "start_time": "2019-11-04T14:10:10.648Z"
    }
   },
   "outputs": [],
   "source": [
    "#proses stemming\n",
    "def stem_list(row):\n",
    "    my_list = row['words']\n",
    "    stemmed_list = [stemmer.stem(word) for word in my_list]\n",
    "    return (stemmed_list)\n",
    "\n",
    "posts['stemmer_words'] = posts.apply(stem_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.173777Z",
     "start_time": "2019-11-04T14:10:10.652Z"
    }
   },
   "outputs": [],
   "source": [
    "#proses stopwords\n",
    "def remove_stops(row):\n",
    "    my_list = row['stemmer_words']\n",
    "    meaningful_words = [w for w in my_list if not w in stopset]\n",
    "    return (meaningful_words)\n",
    "\n",
    "posts['stem_meaningful'] = posts.apply(remove_stops, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.174777Z",
     "start_time": "2019-11-04T14:10:10.655Z"
    }
   },
   "outputs": [],
   "source": [
    "def rejoin_words(row):\n",
    "    my_list = row['stem_meaningful']\n",
    "    joined_words = ( \" \".join(my_list))\n",
    "    return joined_words\n",
    "\n",
    "posts['bow'] = posts.apply(rejoin_words, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.179778Z",
     "start_time": "2019-11-04T14:10:10.660Z"
    }
   },
   "outputs": [],
   "source": [
    "print (posts.shape)\n",
    "posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.187778Z",
     "start_time": "2019-11-04T14:10:10.666Z"
    }
   },
   "outputs": [],
   "source": [
    "for new in posts['bow']:\n",
    "    bow = new.split(' ')\n",
    "    #wordSet= set (new).union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.191778Z",
     "start_time": "2019-11-04T14:10:10.670Z"
    }
   },
   "outputs": [],
   "source": [
    "gabung = ' '.join(posts['bow'])\n",
    "bow = gabung.split(' ')\n",
    "wordSet = set(bow)\n",
    "wordSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.195779Z",
     "start_time": "2019-11-04T14:10:10.673Z"
    }
   },
   "outputs": [],
   "source": [
    "#wordd = []\n",
    "wordDict = dict.fromkeys(wordSet, 0)\n",
    "#wordd.append(wordSet)\n",
    "pd.DataFrame(wordDict, index = posts['stem_meaningful'], columns = wordSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROSES PERHITUNGAN COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.199779Z",
     "start_time": "2019-11-04T14:10:10.703Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "arrayCount = []\n",
    "\n",
    "for new in posts['bow']:\n",
    "    bow = new.split(' ')\n",
    "    wordDict = dict.fromkeys(wordSet, 0)\n",
    "    for word in bow:\n",
    "        wordDict[word]+=1\n",
    "    arrayCount.append(wordDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HASIL PERHITUNGAN COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.205779Z",
     "start_time": "2019-11-04T14:10:10.726Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(arrayCount, columns = wordSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROSES PERHITUNGAN TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.210780Z",
     "start_time": "2019-11-04T14:10:10.750Z"
    }
   },
   "outputs": [],
   "source": [
    "#Perhitungan TF\n",
    "import math\n",
    "arrayTF = []\n",
    "\n",
    "for new in posts['stem_meaningful']:\n",
    "    \n",
    "    termfreq={}\n",
    "    wordDict = dict.fromkeys(wordSet, 0)\n",
    "    for word in new:\n",
    "        wordDict[word]+=1\n",
    "        termfreq[word] = math.log10(1+wordDict[word])\n",
    "    arrayTF.append(termfreq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HASIL PERHITUNGAN TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.214780Z",
     "start_time": "2019-11-04T14:10:10.772Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(arrayTF, columns = wordSet)\n",
    "df\n",
    "df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/markdown"
   },
   "source": [
    "PROSES PERHITUNGAN IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.217780Z",
     "start_time": "2019-11-04T14:10:10.796Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "arrayIDF = []\n",
    "\n",
    "panjang=len(posts[\"bow\"])\n",
    "\n",
    "termglobalfreq = {}\n",
    "for term in wordDict:\n",
    "    count = 0\n",
    "    for new in posts[\"stem_meaningful\"]:\n",
    "        if term in new:\n",
    "            count+=1\n",
    "    termglobalfreq[term] = math.log10(panjang/count)\n",
    "arrayIDF.append(termglobalfreq)\n",
    "    \n",
    "#print(termglobalfreq)\n",
    "#idfDict= math.log10(len(set(new))/panjang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HASIL PERHITUNGAN IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.223780Z",
     "start_time": "2019-11-04T14:10:10.818Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(arrayIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROSES PERHITUNGAN TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.226780Z",
     "start_time": "2019-11-04T14:10:10.841Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "arrayTFIDF = []\n",
    "\n",
    "for new in posts['stem_meaningful']:\n",
    "    \n",
    "    termfreq={}\n",
    "    wordDict = dict.fromkeys(wordSet, 0)\n",
    "    for word in new:\n",
    "        wordDict[word]+=1\n",
    "        termfreq[word] = math.log10(1+wordDict[word]) * termglobalfreq[word]\n",
    "    arrayTFIDF.append(termfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.231781Z",
     "start_time": "2019-11-04T14:10:10.845Z"
    }
   },
   "outputs": [],
   "source": [
    "arrayTFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.235781Z",
     "start_time": "2019-11-04T14:10:10.850Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf = pd.DataFrame(arrayTFIDF, columns=wordSet)\n",
    "tfidf.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.239781Z",
     "start_time": "2019-11-04T14:10:10.854Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "sparse_matrix = sparse.csr_matrix(tfidf.fillna(0).values)\n",
    "print(sparse_matrix)\n",
    "#X = pd.DataFrame(sparse_matrix)\n",
    "#X.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.243781Z",
     "start_time": "2019-11-04T14:10:10.858Z"
    }
   },
   "outputs": [],
   "source": [
    "sparse_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.246782Z",
     "start_time": "2019-11-04T14:10:10.861Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "lsa = TruncatedSVD(n_components=2, n_iter=8)\n",
    "lsa.fit(sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.250782Z",
     "start_time": "2019-11-04T14:10:10.866Z"
    }
   },
   "outputs": [],
   "source": [
    "lsa.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.253782Z",
     "start_time": "2019-11-04T14:10:10.869Z"
    }
   },
   "outputs": [],
   "source": [
    "print(lsa.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.258782Z",
     "start_time": "2019-11-04T14:10:10.874Z"
    }
   },
   "outputs": [],
   "source": [
    "print(lsa.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.262782Z",
     "start_time": "2019-11-04T14:10:10.879Z"
    }
   },
   "outputs": [],
   "source": [
    "print(lsa.explained_variance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.267783Z",
     "start_time": "2019-11-04T14:10:10.885Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.271783Z",
     "start_time": "2019-11-04T14:10:10.889Z"
    }
   },
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.275783Z",
     "start_time": "2019-11-04T14:10:10.897Z"
    }
   },
   "outputs": [],
   "source": [
    "terms = tfidf.fillna(0)\n",
    "for i, comp in enumerate(lsa.components_): \n",
    "    termsInComp = zip (terms,comp)\n",
    "    sortedTerms =  sorted(termsInComp, key=lambda x: x[1], reverse=True) [:15]\n",
    "    print(\"Concept %d:\" % i )\n",
    "    for term in sortedTerms:\n",
    "        print(\"{} {}\".format(term[0], term[1]))\n",
    "    print (\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.278783Z",
     "start_time": "2019-11-04T14:10:10.901Z"
    }
   },
   "outputs": [],
   "source": [
    "lsa.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.281784Z",
     "start_time": "2019-11-04T14:10:10.907Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(posts, tfidf.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.288784Z",
     "start_time": "2019-11-04T14:10:10.912Z"
    }
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<style type=\"text/css\">\n",
    "table.dataframe td, table.dataframe th {\n",
    "    border: 0px  black solid !important;\n",
    "  color: black !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.293784Z",
     "start_time": "2019-11-04T14:10:10.916Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "csr_matrix((3, 3), dtype=np.int8).toarray()\n",
    "\n",
    "docs = arrayTFIDF\n",
    "indptr = [0]\n",
    "indices = []\n",
    "data = []\n",
    "vocabulary = {}\n",
    "for d in docs:\n",
    "    for term in d:\n",
    "        index = vocabulary.setdefault(term, len(vocabulary))\n",
    "        indices.append(index)\n",
    "        data.append(1)\n",
    "    indptr.append(len(indices))\n",
    "\n",
    "csr_matrix((data, indices, indptr), dtype=int).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:10:38.296784Z",
     "start_time": "2019-11-04T14:10:10.920Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer =TfidfVectorizer(sublinear_tf=False, smooth_idf=True, ngram_range=(1, 1))\n",
    "X = vectorizer.fit_transform(posts['bow'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
